method: snn_fd

data:
  input_type: pointcloud
  # Legacy folder-based config (kept for backwards compatibility)
  path: data/ShapeNet
  train_split: train
  val_split: val
  test_split: test
  
  # New HDF5-based dataset config for PUGAN/PU1K
  use_hdf5: true
  hdf5_paths:
    pugan: data/PU1K/release/PUGAN_poisson_256_poisson_1024.h5
    pu1k_train: data/PU1K/release/PU1K/train/pu1k_poisson_256_poisson_1024_pc_2500_patch50_addpugan.h5
  hdf5_input_key: poisson_256
  hdf5_gt_key: poisson_1024
  num_input_points: 256
  num_gt_points: 1024
  
  pointcloud_n: 2048
  patch_n: 100
  patch_k: 16
  pointcloud_noise: 0
  
  pointcloud_file: pointcloud.npz
  fd_file: fd.npz
  
  normalize_points: true
  normalize_scale: 0.85
  normalize_center: true
  normalize_clip: 5.0
  
  use_cache: true
  cache_size: 800

model:
  type: 'enhanced'
  
  # SNN architecture mode
  # false = SNN only in encoder (recommended), true = SNN in both encoder and decoder
  use_snn_decoder: false
  
  # === INCREASED CAPACITY ===
  k: 32                    # was 20 - more neighbors for richer local context
  emb_dims: 768            # was 512 - larger feature embedding
  time_steps_enc: 7        # was 5 - more temporal refinement
  time_steps_dec: 10       # was 8 - more decoder iterations
  
  # Multi-scale k values for encoder - wider range
  k_scales: [8, 16, 32, 48]  # was [10, 20, 40] - added 4th scale for more detail
  
  # EIF neuron parameters for early encoder layers
  eif_params:
    delta_T_init: 1.0
    theta_rh_init: 0.8
  
  num_heads: 8             # was 4 - more attention heads
  dropout: 0.1
  
  decoder_hidden_dims: [384, 256, 128]  # was [256, 128, 64] - wider decoder
  
  snn_params:
    membrane_decay_init: 0.85
    threshold_adapt_init: 0.015
    refractory_decay_init: 0.6
    grad_width: 8.0
    reset_method: 'soft'
    spike_function: 'sigmoid'
    
  init_method: 'xavier_normal'
  init_gain: 0.008
  init_bias: 0.0

training:
  batch_size: 4
  num_workers: 6              # was 2 - faster data loading
  prefetch_factor: 2          # was 1 - prefetch more
  
  lr: 0.0002                  # slightly lower for larger model stability
  lr_policy: 'cosine'
  lr_decay: 0.95
  lr_decay_step: 5000         # was 2500 - slower decay for longer training
  min_lr: 1e-5                # was 1e-7 - keep LR meaningful
  warmup_steps: 2000          # was 1000 - longer warmup for larger model
  warmup_factor: 0.01
  
  optimizer: 'adamw'          # was adam - AdamW better for larger models
  weight_decay: 0.0001        # was 0.00001 - more regularization
  betas: [0.9, 0.999]
  eps: 1e-8
  
  grad_clip: 0.1              # was 0.15 - tighter clipping for stability
  grad_clip_type: 'norm'
  gradient_accumulation: 2    # was 1 - effective batch size 8
  
  max_iterations: 300000      # was 150000 - extended for thorough convergence
  max_epochs: 2000
  
  print_every: 100
  visualize_every: 5000
  checkpoint_every: 2000
  validate_every: 1000
  backup_every: 20000
  
  early_stopping: true
  patience: 30000             # was 15000 - double patience for longer training
  min_delta: 0.0005
  
  use_amp: true               # was false - ENABLE for faster training!
  amp_opt_level: 'O1'
  
  gradient_checkpointing: false
  
  snn_training:
    spike_rate_target: 0.08
    spike_rate_weight: 0.0005
    state_reset_freq: 100
    gradient_scaling: true
    gradient_scale_factor: 0.5

test:
  model_file: 'model_best.pt'
  batch_size: 8
  num_samples: 100
  metrics: ['mae', 'mse', 'smooth_l1']

generation:
  generation_dir: 'test'
  num_generate: 20
  save_predictions: true
  save_visualizations: false

augmentation:
  random_rotation: true
  rotation_axis: 'z'
  random_scale: false
  scale_range: [1.0, 1.0]
  random_jitter: true
  jitter_sigma: 0.0015
  jitter_clip: 0.015
  random_translate: false
  random_dropout: false
  random_mask: false
  normalize_each_batch: true
  unit_sphere_normalize: true

loss:
  beta: 0.1
  reduction: 'mean'
  distance_consistency: false
  consistency_weight: 0.0
  consistency_neighbors: 8
  clip_loss: true
  max_loss_value: 20.0
  min_loss_value: 0.0

monitoring:
  use_tensorboard: true
  log_dir: 'logs'
  use_wandb: false
  track_metrics: ['loss', 'mae', 'mse', 'learning_rate', 'grad_norm']
  visualize_samples: 2
  log_snn_stats: true
  log_spike_rates: true

checkpoint:
  save_dir: 'checkpoints'
  save_best: true
  save_last: true
  max_checkpoints: 4
  resume: true
  resume_file: 'model_best.pt'
  pretrained: false
  save_optimizer_state: true
  save_scheduler_state: true

hardware:
  cudnn_benchmark: true
  deterministic: false
  seed: 42
  pin_memory: true
  non_blocking: true
  gpu_ids: [0]
  torch_compile: false
  reduce_fragmentation: true
  empty_cache_freq: 500
  max_split_size_mb: 32

debug:
  debug_mode: true
  profile: false
  check_nan: true
  check_gradients: true
  log_memory: true
  memory_threshold_mb: 14000
  log_memory_every: 200
  validate_inputs: true
  check_input_range: true
  max_input_value: 25.0
  min_input_value: -25.0

stability:
  use_gradient_accumulation: false
  clip_gradients: true
  gradient_clip_value: 0.15
  gradient_clip_norm: 1.0
  use_learning_rate_warmup: true
  warmup_steps: 1000
  use_learning_rate_decay: true
  initialize_weights: true
  weight_init_scale: 0.008
  use_batch_norm: false
  batch_norm_momentum: 0.1
  use_dropout: true
  dropout_rate: 0.1
  dropout_inplace: false

snn_stability:
  clamp_membrane: true
  membrane_min: -20.0
  membrane_max: 20.0
  soft_spikes_init: true
  harden_spikes_epochs: 10
  surrogate_gradient: 'sigmoid'
  surrogate_scale: 8.0
  reset_states_batch: true
  reset_states_eval: true
  monitor_spike_rates: true
  target_spike_rate: 0.08
  spike_rate_tolerance: 0.03

scheduler:
  type: 'step'
  step_size: 15000            # was 10000 - less frequent decay for longer training
  gamma: 0.5